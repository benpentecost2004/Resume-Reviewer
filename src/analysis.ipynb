{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "45edb5df",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d861217a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pandas in c:\\users\\alexm\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (2.3.3)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\users\\alexm\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from pandas) (2.3.5)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\alexm\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\alexm\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\alexm\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\alexm\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.2 -> 25.3\n",
      "[notice] To update, run: C:\\Users\\alexm\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip3 install pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc573eea",
   "metadata": {},
   "source": [
    "## 1. Data Preperation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8c3b5d82",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alexm\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Role</th>\n",
       "      <th>Resume</th>\n",
       "      <th>Decision</th>\n",
       "      <th>Reason_for_decision</th>\n",
       "      <th>Job_Description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>E-commerce Specialist</td>\n",
       "      <td>Here's a professional resume for Jason Jones:\\...</td>\n",
       "      <td>reject</td>\n",
       "      <td>Lacked leadership skills for a senior position.</td>\n",
       "      <td>Be part of a passionate team at the forefront ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Game Developer</td>\n",
       "      <td>Here's a professional resume for Ann Marshall:...</td>\n",
       "      <td>select</td>\n",
       "      <td>Strong technical skills in AI and ML.</td>\n",
       "      <td>Help us build the next-generation products as ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Human Resources Specialist</td>\n",
       "      <td>Here's a professional resume for Patrick Mccla...</td>\n",
       "      <td>reject</td>\n",
       "      <td>Insufficient system design expertise for senio...</td>\n",
       "      <td>We need a Human Resources Specialist to enhanc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>E-commerce Specialist</td>\n",
       "      <td>Here's a professional resume for Patricia Gray...</td>\n",
       "      <td>select</td>\n",
       "      <td>Impressive leadership and communication abilit...</td>\n",
       "      <td>Be part of a passionate team at the forefront ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>E-commerce Specialist</td>\n",
       "      <td>Here's a professional resume for Amanda Gross:...</td>\n",
       "      <td>reject</td>\n",
       "      <td>Lacked leadership skills for a senior position.</td>\n",
       "      <td>We are looking for an experienced E-commerce S...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         Role  \\\n",
       "0       E-commerce Specialist   \n",
       "1              Game Developer   \n",
       "2  Human Resources Specialist   \n",
       "3       E-commerce Specialist   \n",
       "4       E-commerce Specialist   \n",
       "\n",
       "                                              Resume Decision  \\\n",
       "0  Here's a professional resume for Jason Jones:\\...   reject   \n",
       "1  Here's a professional resume for Ann Marshall:...   select   \n",
       "2  Here's a professional resume for Patrick Mccla...   reject   \n",
       "3  Here's a professional resume for Patricia Gray...   select   \n",
       "4  Here's a professional resume for Amanda Gross:...   reject   \n",
       "\n",
       "                                 Reason_for_decision  \\\n",
       "0    Lacked leadership skills for a senior position.   \n",
       "1              Strong technical skills in AI and ML.   \n",
       "2  Insufficient system design expertise for senio...   \n",
       "3  Impressive leadership and communication abilit...   \n",
       "4    Lacked leadership skills for a senior position.   \n",
       "\n",
       "                                     Job_Description  \n",
       "0  Be part of a passionate team at the forefront ...  \n",
       "1  Help us build the next-generation products as ...  \n",
       "2  We need a Human Resources Specialist to enhanc...  \n",
       "3  Be part of a passionate team at the forefront ...  \n",
       "4  We are looking for an experienced E-commerce S...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"hf://datasets/AzharAli05/Resume-Screening-Dataset/dataset.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "40c56cc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10174, 5)\n",
      "Role    45\n",
      "dtype: int64\n",
      "Reason_for_decision    539\n",
      "dtype: int64\n",
      "0.49734617652840574\n"
     ]
    }
   ],
   "source": [
    "from pandasql import sqldf\n",
    "\n",
    "all_roles_query = \"\"\"\n",
    "SELECT DISTINCT Role\n",
    "FROM df \n",
    "\"\"\"\n",
    "all_decision_reason_query = \"\"\"\n",
    "SELECT DISTINCT Reason_for_decision\n",
    "FROM df \n",
    "\"\"\"\n",
    "print(df.shape)\n",
    "all_roles = sqldf(all_roles_query, locals())\n",
    "all_dec_reason = sqldf(all_decision_reason_query, locals())\n",
    "print(all_roles.count())\n",
    "print(all_dec_reason.count())\n",
    "df['decision_binary'] = df['Decision'].apply(lambda x: 1 if x == 'select' else 0)\n",
    "print(df['decision_binary'].mean())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2597322d",
   "metadata": {},
   "source": [
    "## Observations\n",
    "This dataset includes recruiting decisions for 45 different roles and consists of over 10,000 decisions made. Each candidate entry hase a role they applied for, their resume in a text format, the decision made by the recruiter, the reason why they made that decision, and the job description.\n",
    "This dataset also has a balanced set of outcomes with about 50% of each outcome."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3906abe",
   "metadata": {},
   "source": [
    "## Data Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f6fb0e2",
   "metadata": {},
   "source": [
    "- Personal Info\n",
    "  - Every resume starts with ... resume for {Name}\n",
    "  - Look for keywords email,phone,LinkedIn,address\n",
    "- Data Splitting\n",
    "  - Split resume into different chuncks (Summary, education, skills, experience, achievements, certifications, projects, references, professional memberships)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2be9354",
   "metadata": {},
   "source": [
    "### 1. Add tailored columns for each section of the resume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffd73a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def extract_sections_by_headers(text):\n",
    "    header_map = {\n",
    "        # Summary variants\n",
    "        \"summary\": \"summary\",\n",
    "        \"objective\": \"summary\",\n",
    "        \"about me\": \"summary\",\n",
    "        \"career focus\": \"summary\",\n",
    "        \"career goals\": \"summary\",\n",
    "        \"professional summary\": \"summary\",\n",
    "\n",
    "        # Experience variants\n",
    "        \"experience\": \"experience\",\n",
    "        \"professional experience\": \"experience\",\n",
    "        \"employment history\": \"experience\",\n",
    "        \"work history\": \"experience\",\n",
    "\n",
    "        # Skills variants\n",
    "        \"skills\": \"skills\",\n",
    "        \"technical skills\": \"skills\",\n",
    "        \"core competencies\": \"skills\",\n",
    "\n",
    "        # Education variants\n",
    "        \"education\": \"education\",\n",
    "        \"academic background\": \"education\",\n",
    "\n",
    "        # Certifications variants\n",
    "        \"certifications\": \"certifications\",\n",
    "        \"licenses\": \"certifications\",\n",
    "\n",
    "        # Projects variants\n",
    "        \"projects\": \"projects\",\n",
    "        \"personal projects\": \"projects\",\n",
    "        \"portfolio\": \"projects\",\n",
    "\n",
    "        # References variants\n",
    "        \"references\": \"references\",\n",
    "        \"referees\": \"references\",\n",
    "        \"recommendations\": \"references\",\n",
    "        \"professional references\": \"references\",\n",
    "    }\n",
    "\n",
    "    # Compile regex to find headers exactly matching the keys\n",
    "    # allow colon/dash and surrounding whitespace\n",
    "    pattern = re.compile(\n",
    "        r'^\\s*(%s)\\s*[:\\-]?\\s*$' % '|'.join(re.escape(h) for h in header_map.keys()),\n",
    "        flags=re.IGNORECASE | re.MULTILINE\n",
    "    )\n",
    "\n",
    "    matches = list(pattern.finditer(text))\n",
    "    sections = {}\n",
    "\n",
    "    for i, match in enumerate(matches):\n",
    "        raw_header = match.group(1).lower()\n",
    "        canonical_header = header_map[raw_header]\n",
    "\n",
    "        start = match.end()\n",
    "        end = matches[i + 1].start() if i + 1 < len(matches) else len(text)\n",
    "\n",
    "        section_text = text[start:end].strip()\n",
    "\n",
    "        # Normalize whitespace inside section text (optional)\n",
    "        section_text = re.sub(r'\\n{3,}', '\\n\\n', section_text)  # collapse 3+ newlines to 2\n",
    "        section_text = re.sub(r'[ \\t]+', ' ', section_text)      # collapse spaces/tabs to one space\n",
    "\n",
    "        if canonical_header in sections:\n",
    "            # Separate multiple blocks with two newlines for readability\n",
    "            sections[canonical_header] += \"\\n\\n\" + section_text\n",
    "        else:\n",
    "            sections[canonical_header] = section_text\n",
    "\n",
    "    # Ensure all expected keys are present even if empty\n",
    "    for key in set(header_map.values()):\n",
    "        sections.setdefault(key, \"\")\n",
    "\n",
    "    return sections\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0cc6dc95",
   "metadata": {},
   "outputs": [],
   "source": [
    "section_keys = [\n",
    "    \"summary\",\n",
    "    \"experience\",\n",
    "    \"skills\",\n",
    "    \"education\",\n",
    "    \"certifications\",\n",
    "    \"projects\",\n",
    "    \"references\"\n",
    "]\n",
    "\n",
    "# Remove existing extracted columns if they exist\n",
    "for col in section_keys:\n",
    "    if col in df.columns:\n",
    "        df.drop(columns=[col], inplace=True)\n",
    "\n",
    "# Extract and build new DataFrame of sections\n",
    "extracted_data = []\n",
    "for index, row in df.iterrows():\n",
    "    sections = extract_sections_by_headers(row['Resume'])\n",
    "    row_data = {key: sections.get(key, \"\") for key in section_keys}\n",
    "    extracted_data.append(row_data)\n",
    "\n",
    "extracted_df = pd.DataFrame(extracted_data)\n",
    "\n",
    "# Concatenate with original DataFrame\n",
    "df = pd.concat([df.reset_index(drop=True), extracted_df], axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac3fbd0d",
   "metadata": {},
   "source": [
    "### 2. Data Cleaning\n",
    "- Remove bullet points\n",
    "- lowercase letters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "29c23806",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Function to clean bullet points from text\n",
    "def clean_text(text):\n",
    "    # Common bullet point regex\n",
    "    bullet_chars = [\n",
    "        r'\\*',  # asterisk\n",
    "        r'-',   # dash\n",
    "        r'\\+',  # plus\n",
    "        r'\\u2022',  # bullet •\n",
    "        r'\\u2023',  # triangular bullet ‣\n",
    "        r'\\u25E6',  # white bullet ◦\n",
    "        r'\\u2043',  # hyphen bullet ⁃\n",
    "        r'\\u2219',  # bullet operator ∙\n",
    "        r'\\u00B7'   # middle dot ·\n",
    "    ]\n",
    "    bullet_pattern = '[' + ''.join(bullet_chars) + ']' + r'\\s*'\n",
    "    if not isinstance(text, str):\n",
    "        return text  # skip non-string values\n",
    "    cleaned_text = re.sub(bullet_pattern, '', text)\n",
    "    cleaned_text.lower()\n",
    "    return cleaned_text\n",
    "columns_to_clean = ['summary', 'experience', 'skills', 'education', 'certifications', 'projects', 'references']\n",
    "\n",
    "for col in columns_to_clean:\n",
    "    df[col] = df[col].apply(clean_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "314e93b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10174\n",
      "Role:\n",
      "Data Engineer\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Decision:\n",
      "select\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Reason_for_decision:\n",
      "Solid experience in machine learning and AI.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Job_Description:\n",
      "Join our fast-growing team and help us scale our product offerings as a Data Engineer with expertise in Airflow, Data Warehousing, MLOps.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "decision_binary:\n",
      "1\n",
      "----------------------------------------------------------------------------------------------------\n",
      "summary:\n",
      "Highly skilled Data Engineer with expertise in MLOps, Airflow, Big Data, Cloud Platforms, and Spark. Proven track record of designing, building, and deploying scalable data pipelines and machine learning models that drive business value. Proficient in collaborating with cross-functional teams to drive data-driven decision-making.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "experience:\n",
      "Senior Data Engineer, ABC Corporation (2018-Present)\n",
      "\n",
      "Designed and implemented scalable data pipelines using Apache Airflow, Spark, and Python, resulting in 30% reduction in data processing time and 25% improvement in model accuracy.\n",
      "Collaborated with data scientists to develop and deploy machine learning models using TensorFlow and PyTorch, achieving 20% increase in model performance and 15% reduction in model deployment time.\n",
      "Worked with cloud teams to design and deploy cloud-agnostic data platforms on AWS and GCP, ensuring 99.99% uptime and 30% reduction in cloud costs.\n",
      "Led a team of 3 data engineers to design and implement a data lake on Apache Hadoop, resulting in 40% reduction in data storage costs and 20% improvement in data query performance.\n",
      "\n",
      "Data Engineer, DEF Startups (2015-2018)\n",
      "\n",
      "Designed and implemented real-time data pipelines using Apache Kafka, Spark, and Python, resulting in 20% improvement in data freshness and 15% reduction in data processing time.\n",
      "Collaborated with data analysts to develop and deploy data visualizations using Tableau and Power BI, achieving 25% increase in data insights and 15% reduction in reporting time.\n",
      "Worked with cloud teams to design and deploy cloud-based data platforms on AWS and Azure, ensuring 99.99% uptime and 20% reduction in cloud costs.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "skills:\n",
      "Programming languages: Python, Scala, Java\n",
      "Data engineering tools: Apache Airflow, Apache Spark, Apache Kafka\n",
      "Cloud platforms: AWS, GCP, Azure\n",
      "Big data platforms: Apache Hadoop, Apache HBase\n",
      "Data visualization tools: Tableau, Power BI\n",
      "Machine learning frameworks: TensorFlow, PyTorch\n",
      "Operating Systems: Linux, Windows\n",
      "----------------------------------------------------------------------------------------------------\n",
      "education:\n",
      "Master of Science in Data Science, XYZ University (2015)\n",
      "Bachelor of Science in Computer Science, ABC University (2010)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "certifications:\n",
      "AWS Certified Data Engineer (2019)\n",
      "Google Cloud Certified - Professional Data Engineer (2020)\n",
      "\n",
      "Achievements:\n",
      "\n",
      "Winner, ABC Corporation's Data Science Competition (2019): Developed a machine learning model using TensorFlow and PyTorch that achieved 95% accuracy and 10% improvement in model performance.\n",
      "Speaker, DEF Startups' Data Engineering Conference (2018): Presented a talk on \"Designing Scalable Data Pipelines using Apache Airflow and Spark\".\n",
      "----------------------------------------------------------------------------------------------------\n",
      "projects:\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "references:\n",
      "Available upon request.\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(len(df))\n",
    "first_row = df.iloc[155]\n",
    "\n",
    "for col, val in first_row.items():\n",
    "    if col != \"Resume\":\n",
    "      print(f\"{col}:\\n{val}\\n{'-'*100}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61d93bd4",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b29bacb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['combined_text'] = (\n",
    "    df['summary'].fillna('') + ' ' +\n",
    "    df['experience'].fillna('') + ' ' +\n",
    "    df['skills'].fillna('') + ' ' +\n",
    "    df['education'].fillna('') + ' ' +\n",
    "    df['certifications'].fillna('') + ' ' +\n",
    "    df['projects'].fillna('') + ' ' +\n",
    "    df['Reason_for_decision'].fillna('') + ' ' +\n",
    "    df['Job_Description'].fillna('')\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9cf6c012",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import ComplementNB\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "def train_and_evaluate(df, \n",
    "                       max_features=30000, \n",
    "                       ngram_range=(3,4), \n",
    "                       k=1500, \n",
    "                       alpha=0.01,\n",
    "                       random_state=42,\n",
    "                       test_size=0.2,\n",
    "                       return_probs=False):\n",
    "    \"\"\"\n",
    "    Train and evaluate ComplementNB with given parameters.\n",
    "    If return_probs=True, also return test DataFrame with predicted probabilities per sample.\n",
    "    \n",
    "    Returns:\n",
    "        metrics (dict): Performance metrics\n",
    "        model: trained ComplementNB model\n",
    "        vectorizer: fitted TfidfVectorizer\n",
    "        chi2_selector: fitted SelectKBest\n",
    "        (optional) X_test_with_probs (DataFrame): test samples with predicted probabilities\n",
    "        (optional) y_proba (np.array): predicted probabilities for test set\n",
    "    \"\"\"\n",
    "\n",
    "    # Vectorize text\n",
    "    vectorizer = TfidfVectorizer(stop_words='english', max_features=max_features, ngram_range=ngram_range)\n",
    "    X_text = (\n",
    "        df[\"combined_text\"].fillna(\"\") + \" \" +\n",
    "        df[\"Job_Description\"].fillna(\"\") + \" \" +\n",
    "        df[\"Role\"].fillna(\"\")\n",
    "    )\n",
    "    X = vectorizer.fit_transform(X_text)\n",
    "    y = df['decision_binary']\n",
    "\n",
    "    # Feature selection with fixed k\n",
    "    current_k = min(k, X.shape[1])\n",
    "    chi2_selector = SelectKBest(chi2, k=current_k)\n",
    "    X_selected = chi2_selector.fit_transform(X, y)\n",
    "\n",
    "    # Train/test split - split original df to keep roles for later\n",
    "    df_train, df_test, X_train, X_test, y_train, y_test = train_test_split(\n",
    "        df, X_selected, y, test_size=test_size, random_state=random_state, stratify=y\n",
    "    )\n",
    "\n",
    "    # Train model\n",
    "    model = ComplementNB(alpha=alpha)\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Predict and evaluate\n",
    "    y_pred = model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(y_test, y_pred, average='macro')\n",
    "\n",
    "    metrics = {\n",
    "        'max_features': max_features,\n",
    "        'ngram_range': ngram_range,\n",
    "        'k': current_k,\n",
    "        'alpha': alpha,\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1\n",
    "    }\n",
    "\n",
    "    if return_probs:\n",
    "        y_proba = model.predict_proba(X_test)[:, 1]  # prob of positive class\n",
    "        df_test = df_test.copy()\n",
    "        df_test['predicted_proba'] = y_proba\n",
    "        return metrics, model, vectorizer, chi2_selector, df_test, y_proba\n",
    "    else:\n",
    "        return metrics, model, vectorizer, chi2_selector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "86ccbe8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_metric_by_param(results, param_name, param_values=None):\n",
    "    \"\"\"\n",
    "    Plot all metrics vs param_name.\n",
    "    results: list of dicts with metrics and params\n",
    "    param_name: str, which param to group by and plot on x-axis\n",
    "    param_values: optional, list of param values to order the plot\n",
    "    \"\"\"\n",
    "\n",
    "    metrics = ['accuracy', 'precision', 'recall', 'f1']\n",
    "\n",
    "    if param_values is None:\n",
    "        param_values = sorted(set(res[param_name] for res in results))\n",
    "\n",
    "    metric_data = {metric: [] for metric in metrics}\n",
    "\n",
    "    for val in param_values:\n",
    "        matching = [res for res in results if res[param_name] == val]\n",
    "        if matching:\n",
    "            for metric in metrics:\n",
    "                avg_metric = sum(res[metric] for res in matching) / len(matching)\n",
    "                metric_data[metric].append(avg_metric)\n",
    "        else:\n",
    "            for metric in metrics:\n",
    "                metric_data[metric].append(None)\n",
    "\n",
    "    # Plot all metrics vs param on same plot\n",
    "    plt.figure(figsize=(10,6))\n",
    "    for metric in metrics:\n",
    "        plt.plot(param_values, metric_data[metric], marker='o', label=metric.capitalize())\n",
    "\n",
    "    plt.title(f'Model Performance Metrics vs {param_name}')\n",
    "    plt.xlabel(param_name)\n",
    "    plt.ylabel('Score')\n",
    "    plt.ylim(0, 1)\n",
    "    plt.grid(True, axis='y')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Plot different metrics for models\n",
    "# # Parameter grids\n",
    "# max_features_list = [1000, 5000, 10000, 15000, 20000, 25000, 30000]\n",
    "# ngram_ranges = [(1,2), (1,3), (1,4), (2,3), (3,4)]\n",
    "# k_values = [100, 500, 1000, 1500, 2000]\n",
    "# alphas = [0.01]\n",
    "\n",
    "# results = []\n",
    "\n",
    "# for max_feat in max_features_list:\n",
    "#     for ngram in ngram_ranges:\n",
    "#         for k in k_values:\n",
    "#             for alpha in alphas:\n",
    "#                 print(f\"Training with max_features={max_feat}, ngram_range={ngram}, k={k}, alpha={alpha}\")\n",
    "#                 res = train_and_evaluate(\n",
    "#                     df,\n",
    "#                     max_features=max_feat,\n",
    "#                     ngram_range=ngram,\n",
    "#                     k=k,\n",
    "#                     alpha=alpha,\n",
    "#                     random_state=42,\n",
    "#                     test_size=0.2\n",
    "#                 )\n",
    "#                 results.append(res)\n",
    "\n",
    "# # Convert ngram_range tuples to string for nicer plotting labels\n",
    "# # Convert ngram_range tuples to string for nicer plotting labels\n",
    "# for r in results:\n",
    "#     r['ngram_range_str'] = str(r['ngram_range'])\n",
    "\n",
    "# # Print parameter ranges tested\n",
    "# print(\"\\nParameter ranges tested:\")\n",
    "# for param in ['max_features', 'k', 'alpha']:\n",
    "#     values = sorted(set(res[param] for res in results))\n",
    "#     print(f\"{param}: min={values[0]}, max={values[-1]}\")\n",
    "\n",
    "# ngram_vals = sorted(set(res['ngram_range_str'] for res in results))\n",
    "# print(f\"ngram_range options: {', '.join(ngram_vals)}\")\n",
    "\n",
    "# # Print best combination by accuracy\n",
    "# best_result = max(results, key=lambda x: x['accuracy'])\n",
    "# print(\"\\nBest combination found:\")\n",
    "# for key, value in best_result.items():\n",
    "#     if key != 'ngram_range_str':  # optional, skip to reduce clutter\n",
    "#         print(f\"{key}: {value}\")\n",
    "\n",
    "# # Plot grouped comparisons\n",
    "# plot_metric_by_param(results, 'max_features')\n",
    "# plot_metric_by_param(results, 'ngram_range_str')\n",
    "# plot_metric_by_param(results, 'k')\n",
    "# plot_metric_by_param(results, 'alpha')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bc8d8474",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validated metrics (cv=5):\n",
      "Accuracy:  0.9456\n",
      "Precision: 0.9458\n",
      "Recall:    0.9456\n",
      "F1 Score:  0.9456\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.naive_bayes import ComplementNB\n",
    "\n",
    "def cross_val_evaluate_best(df, max_features, ngram_range, k, alpha, cv=5, random_state=42):\n",
    "    skf = StratifiedKFold(n_splits=cv, shuffle=True, random_state=random_state)\n",
    "\n",
    "    # NEW → combine resume + job role + job desc\n",
    "    X_text = (\n",
    "        df[\"combined_text\"].fillna(\"\") + \" \" +\n",
    "        df[\"Job_Description\"].fillna(\"\") + \" \" +\n",
    "        df[\"Role\"].fillna(\"\")\n",
    "    )\n",
    "    y = df[\"decision_binary\"]\n",
    "\n",
    "    accuracies, precisions, recalls, f1s = [], [], [], []\n",
    "\n",
    "    for train_idx, val_idx in skf.split(X_text, y):\n",
    "\n",
    "        X_train_text = X_text.iloc[train_idx]\n",
    "        X_val_text = X_text.iloc[val_idx]\n",
    "        y_train = y.iloc[train_idx]\n",
    "        y_val = y.iloc[val_idx]\n",
    "\n",
    "        # Vectorizer\n",
    "        vectorizer = TfidfVectorizer(\n",
    "            stop_words=\"english\",\n",
    "            max_features=max_features,\n",
    "            ngram_range=ngram_range\n",
    "        )\n",
    "        X_train_tfidf = vectorizer.fit_transform(X_train_text)\n",
    "\n",
    "        # Feature Selection\n",
    "        selector = SelectKBest(chi2, k=min(k, X_train_tfidf.shape[1]))\n",
    "        X_train_sel = selector.fit_transform(X_train_tfidf, y_train)\n",
    "\n",
    "        # Model\n",
    "        clf = ComplementNB(alpha=alpha)\n",
    "        clf.fit(X_train_sel, y_train)\n",
    "\n",
    "        # Validation transform\n",
    "        X_val_tfidf = vectorizer.transform(X_val_text)\n",
    "        X_val_sel = selector.transform(X_val_tfidf)\n",
    "\n",
    "        y_pred = clf.predict(X_val_sel)\n",
    "\n",
    "        # Metrics\n",
    "        accuracies.append(accuracy_score(y_val, y_pred))\n",
    "        precisions.append(precision_score(y_val, y_pred, average=\"weighted\"))\n",
    "        recalls.append(recall_score(y_val, y_pred, average=\"weighted\"))\n",
    "        f1s.append(f1_score(y_val, y_pred, average=\"weighted\"))\n",
    "\n",
    "    print(f\"Cross-validated metrics (cv={cv}):\")\n",
    "    print(f\"Accuracy:  {sum(accuracies)/cv:.4f}\")\n",
    "    print(f\"Precision: {sum(precisions)/cv:.4f}\")\n",
    "    print(f\"Recall:    {sum(recalls)/cv:.4f}\")\n",
    "    print(f\"F1 Score:  {sum(f1s)/cv:.4f}\")\n",
    "\n",
    "cross_val_evaluate_best(\n",
    "    df,\n",
    "    max_features=30000,\n",
    "    ngram_range=(3,4),\n",
    "    k=1500,\n",
    "    alpha=0.01,\n",
    "    cv=5\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8e7a4aa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted label: 0\n",
      "Probabilities: Not hired = 0.518, Hired = 0.482\n"
     ]
    }
   ],
   "source": [
    "# Extract data from resume\n",
    "from resumeReviewer import extract_text_from_pdf\n",
    "resume_path = \"resume.pdf\"\n",
    "new_resume_text = extract_text_from_pdf(resume_path)\n",
    "\n",
    "# Format resume, job desc, job role \n",
    "resume_sections = extract_sections_by_headers(new_resume_text)\n",
    "job_title = \"Cloud Engineer\"\n",
    "job_desc = \"We're seeking a talented Cloud Engineer to work on AI model development and bring new ideas to life.\"\n",
    "cleaned_sections = {}\n",
    "for section, text in resume_sections.items():\n",
    "    cleaned_sections[section] = clean_text(text)\n",
    "\n",
    "# Format \n",
    "combined_text = (\n",
    "    cleaned_sections.get(\"summary\", \"\") + \" \" +\n",
    "    cleaned_sections.get(\"experience\", \"\") + \" \" +\n",
    "    cleaned_sections.get(\"skills\", \"\") + \" \" +\n",
    "    cleaned_sections.get(\"education\", \"\") + \" \" +\n",
    "    cleaned_sections.get(\"certifications\", \"\") + \" \" +\n",
    "    cleaned_sections.get(\"projects\", \"\") + \" \" +\n",
    "    job_desc + \" \" +\n",
    "    job_title\n",
    ").strip()\n",
    "\n",
    "# Make prediction of outcome\n",
    "def predict_resume_text(model, vectorizer, chi2_selector, combined_text):\n",
    "    # Convert to TF-IDF\n",
    "    X_tfidf = vectorizer.transform([combined_text])\n",
    "    # Apply feature selection\n",
    "    X_selected = chi2_selector.transform(X_tfidf)\n",
    "\n",
    "    # Predict label\n",
    "    label = model.predict(X_selected)[0]\n",
    "\n",
    "    # Probabilities\n",
    "    probs = model.predict_proba(X_selected)[0]\n",
    "\n",
    "    return label, probs\n",
    "    \n",
    "def assign_verdict_and_reason(label, probs):\n",
    "    verdict = \"Hired\" if label == 1 else \"Not hired\"\n",
    "    confidence = probs[label]  # probability of predicted class\n",
    "    reason = f\"Predicted as '{verdict}' with confidence {confidence:.2%}.\"\n",
    "    return verdict, reason\n",
    "\n",
    "metrics, model, vectorizer, chi2_selector = train_and_evaluate(df, ngram_range=(1,3))\n",
    "label, probs = predict_resume_text(model, vectorizer, chi2_selector, combined_text)\n",
    "verdict, reason_for_decision = assign_verdict_and_reason(label, probs)\n",
    "\n",
    "from resumeReviewer import extract_text_from_pdf\n",
    "label, probs = predict_resume_text(model, vectorizer, chi2_selector, new_resume_text)\n",
    "print(f\"Predicted label: {label}\")\n",
    "print(f\"Probabilities: Not hired = {probs[0]:.3f}, Hired = {probs[1]:.3f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cf468597",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top features favoring Hired:\n",
      "2018 def: 8.0722\n",
      "achievements winner ux: 8.0722\n",
      "ability communicate complex: 8.0722\n",
      "achievements best paper: 8.0722\n",
      "2018 data science: 8.0704\n",
      "20xx: 8.0146\n",
      "2022: 8.0146\n",
      "2017 professional: 8.0146\n",
      "2017 professional memberships: 8.0146\n",
      "2016 insufficient design: 8.0146\n",
      "\n",
      "Top features favoring Not Hired:\n",
      "25 reduction customer: -8.2234\n",
      "30 increase efficiency: -8.2124\n",
      "2017 lacks: -8.0932\n",
      "achieving 20 improvement: -8.0932\n",
      "achieving 25: -8.0932\n",
      "achieving 25 increase: -8.0932\n",
      "2018 published article: -8.0932\n",
      "30 increase employee: -8.0904\n",
      "actionable insights drive: -8.0904\n",
      "30 designed: -8.0904\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Get indices of top features with highest difference in log-prob between classes\n",
    "class0_log_prob = model.feature_log_prob_[0]\n",
    "class1_log_prob = model.feature_log_prob_[1]\n",
    "diff = class1_log_prob - class0_log_prob  # positive = favors class 1 (hired)\n",
    "\n",
    "# Sort features by difference\n",
    "top_pos_indices = np.argsort(diff)[-10:]  # top 10 features favoring class 1\n",
    "top_neg_indices = np.argsort(diff)[:10]   # top 10 features favoring class 0\n",
    "print(\"Top features favoring Hired:\")\n",
    "for idx in reversed(top_pos_indices):\n",
    "    print(f\"{feature_names[idx]}: {diff[idx]:.4f}\")\n",
    "\n",
    "print(\"\\nTop features favoring Not Hired:\")\n",
    "for idx in top_neg_indices:\n",
    "    print(f\"{feature_names[idx]}: {diff[idx]:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cf721c14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted hiring probability for E-commerce Specialist: 0.488\n",
      "Predicted hiring probability for Game Developer: 0.487\n",
      "Predicted hiring probability for Human Resources Specialist: 0.488\n",
      "Predicted hiring probability for Mobile App Developer: 0.487\n",
      "Predicted hiring probability for UX Designer: 0.492\n",
      "Predicted hiring probability for Cloud Engineer: 0.486\n",
      "Predicted hiring probability for Digital Marketing Specialist: 0.489\n",
      "Predicted hiring probability for AI Researcher: 0.490\n",
      "Predicted hiring probability for UI Engineer: 0.487\n",
      "Predicted hiring probability for AR/VR Developer: 0.487\n",
      "Predicted hiring probability for Machine Learning Engineer: 0.483\n",
      "Predicted hiring probability for Database Administrator: 0.486\n",
      "Predicted hiring probability for Data Engineer: 0.486\n",
      "Predicted hiring probability for Cybersecurity Analyst: 0.487\n",
      "Predicted hiring probability for Robotics Engineer: 0.488\n",
      "Predicted hiring probability for Business Analyst: 0.487\n",
      "Predicted hiring probability for Data Analyst: 0.487\n",
      "Predicted hiring probability for Cloud Architect: 0.487\n",
      "Predicted hiring probability for Data Architect: 0.487\n",
      "Predicted hiring probability for QA Engineer: 0.481\n",
      "Predicted hiring probability for System Administrator: 0.487\n",
      "Predicted hiring probability for DevOps Engineer: 0.487\n",
      "Predicted hiring probability for Product Manager: 0.487\n",
      "Predicted hiring probability for Data Scientist: 0.487\n",
      "Predicted hiring probability for Full Stack Developer: 0.492\n",
      "Predicted hiring probability for Blockchain Developer: 0.489\n",
      "Predicted hiring probability for Software Engineer: 0.487\n",
      "Predicted hiring probability for Content Writer: 0.487\n",
      "Predicted hiring probability for IT Support Specialist: 0.492\n",
      "Predicted hiring probability for UI Designer: 0.487\n",
      "Predicted hiring probability for Cybersecurity Specialist: 0.488\n",
      "Predicted hiring probability for HR Specialist: 0.488\n",
      "Predicted hiring probability for Network Engineer: 0.487\n",
      "Predicted hiring probability for Graphic Designer: 0.487\n",
      "Predicted hiring probability for UI/UX Designer: 0.492\n",
      "Predicted hiring probability for AI Engineer: 0.490\n",
      "Predicted hiring probability for Project Manager: 0.487\n",
      "Predicted hiring probability for Software Developer: 0.487\n",
      "Predicted hiring probability for product manager: 0.487\n",
      "Predicted hiring probability for software engineer: 0.487\n",
      "Predicted hiring probability for data engineer: 0.486\n",
      "Predicted hiring probability for ui engineer: 0.487\n",
      "Predicted hiring probability for data scientist: 0.487\n",
      "Predicted hiring probability for data analyst: 0.487\n",
      "Predicted hiring probability for ui designer: 0.487\n"
     ]
    }
   ],
   "source": [
    "def predict_hire_probability(candidate_text, job_description, role, vectorizer, chi2_selector, model):\n",
    "    # Combine input text exactly as training\n",
    "    combined_text = f\"{candidate_text} {job_description} {role}\"\n",
    "\n",
    "    # Vectorize\n",
    "    X_vect = vectorizer.transform([combined_text])\n",
    "\n",
    "    # Feature select\n",
    "    X_selected = chi2_selector.transform(X_vect)\n",
    "\n",
    "    # Predict probability of hired (class 1)\n",
    "    proba = model.predict_proba(X_selected)[:, 1][0]\n",
    "\n",
    "    return proba\n",
    "roles_list = all_roles['Role'].tolist()\n",
    "\n",
    "for role in roles_list:\n",
    "    probability = predict_hire_probability(combined_text, job_desc, role, vectorizer, chi2_selector, model)\n",
    "    print(f\"Predicted hiring probability for {role}: {probability:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "23cbdee4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model, vectorizer, and selector successfully exported!\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "# Save trained model\n",
    "with open(\"model.pkl\", \"wb\") as f:\n",
    "    pickle.dump(model, f)\n",
    "\n",
    "# Save vectorizer\n",
    "with open(\"vectorizer.pkl\", \"wb\") as f:\n",
    "    pickle.dump(vectorizer, f)\n",
    "\n",
    "# Save chi2 selector\n",
    "with open(\"chi2_selector.pkl\", \"wb\") as f:\n",
    "    pickle.dump(chi2_selector, f)\n",
    "\n",
    "print(\"Model, vectorizer, and selector successfully exported!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
